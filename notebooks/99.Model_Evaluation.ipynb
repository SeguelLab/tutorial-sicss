{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be40c639",
   "metadata": {},
   "source": [
    "# Evaluating Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f7317b",
   "metadata": {},
   "source": [
    "Since we are dealing with classification models, where the output is a class/label, we will use the most common evaluation metrics\n",
    "used in model evaluation in AI:\n",
    "* Precision\n",
    "* Recall\n",
    "* F1-Score\n",
    "* Accuracy\n",
    "* Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beae662",
   "metadata": {},
   "source": [
    "The **confusion matrix** is a table that compares the predicted labels to the actual ones.\n",
    "In our case we are dealing with binary classification, so it will be a 2x2 matrix.\n",
    "\n",
    "<img src=\"https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/662c42677529a0f4e97e4f96_644aea65cefe35380f198a5a_class_guide_cm08.png\" width=\"500px\">\n",
    "\n",
    "source: https://www.evidentlyai.com/classification-metrics/confusion-matrix\n",
    "\n",
    "The above example shows how the table is structured. The predicted labels on the horizontal, and actual labels on the vertical.\n",
    "* The **True Positives (TP)** are where the model predicted the label as positive and the actual was positive.\n",
    "* The **False Positives (FP)** are where the model predicted the label as positive, but the actual was negative.\n",
    "* The **False Negatives (FN)** are where the model predicted the label as negative, but it was actually positive.\n",
    "* The **True Negatives (TN)** are where the model predicted the label as negative, and it was actually negative.\n",
    "\n",
    "We need the these values from the confusion matrix in order to calculate **precision, recall, f1-score, and accuracy.**\n",
    "\n",
    "An easier way to remember is like this:\n",
    "* TP - Predicted positive, actually positive.\n",
    "* FP - Predicted positive, actually negative.\n",
    "* TN - Predicted negative, actually negative.\n",
    "* FN - Predicted negative, actually negative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f2534c",
   "metadata": {},
   "source": [
    "## 1. Getting the confusion matrix\n",
    "\n",
    "The easiest way is to use the confusion_matrix function from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d12a9bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[51, 49],\n",
       "       [48, 52]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# confusion_matrix has two parameters, the actual labels and predicted.\n",
    "# lets use some random labels from numpy to test.\n",
    "\n",
    "np.random.seed(42)\n",
    "random_actual = np.random.randint(0,2,size=200)\n",
    "random_predicted = np.random.randint(0,2,size=200)\n",
    "\n",
    "conf = confusion_matrix(y_true=random_actual,\n",
    "                 y_pred=random_predicted)\n",
    "conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecdc3ef",
   "metadata": {},
   "source": [
    "In sklearn's confusion_matrix function, it treats the labels as numbers in ascending order, so it orders them left to right, and top down.\n",
    "\n",
    "So the way the confusion matrix is interpreted is slightly different from the image above.\n",
    "\n",
    "This is the way it outputs\n",
    "```python\n",
    "                    Predicted\n",
    "                    Negative(0) Positive(1)\n",
    "\n",
    "Actual   Negative(0)     TN          FP\n",
    "         Positive(1)     FN          TP\n",
    "\n",
    "```\n",
    "Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a86faa",
   "metadata": {},
   "source": [
    "## 2. Calculating metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fc673d",
   "metadata": {},
   "source": [
    "Now that we have the confusion matrix, we can now calculate precision, recall, f1-score, and accuracy.\n",
    "\n",
    "Formulas:\n",
    "\n",
    "```markdown\n",
    "Metrics for positive labels:\n",
    "    Precision = TP / (TP+FP)\n",
    "    Recall = TP / (TP+FN)\n",
    "\n",
    "Metrics for negative labels:\n",
    "    Precision = TN / (TN+FN)\n",
    "    Recall = TN / (TN+FP)\n",
    "\n",
    "F1-score = (2 * P * R) / (P + R)\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "```\n",
    "\n",
    "You can get more resources here: https://medium.com/analytics-vidhya/confusion-matrix-accuracy-precision-recall-f1-score-ade299cf63cd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca91820",
   "metadata": {},
   "source": [
    "\n",
    "```markdown\n",
    "**[Note]** \n",
    "By default when doing binary classification, sklearn will calculate the precision, recall, and f1-score for only the positive label. \n",
    "This is because these functions are made to go beyond just 2 labels, and when the labels are 3 or more, instead it returns an average precision, recall, and f1_score.\n",
    "For the purpose of this demonstration we will calculate these metrics only for the positive labels.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc20ddb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5149\n",
      "Recall: 0.5200\n",
      "F1-score: 0.5174\n",
      "Accuracy: 0.5150\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "precision = precision_score(y_true=random_actual, y_pred=random_predicted)\n",
    "recall = recall_score(y_true=random_actual, y_pred=random_predicted)\n",
    "f1 = f1_score(y_true=random_actual, y_pred=random_predicted)\n",
    "acc = accuracy_score(y_true=random_actual, y_pred=random_predicted)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ef1197",
   "metadata": {},
   "source": [
    "Another function that will also neatly display these metrics is called `classification_report`\n",
    "You will notice it calculates all the metrics for each individual class/label, and also calculates the averages.\n",
    "\n",
    "* Macro avg - Normal average\n",
    "* Weighted avg - Average that considers the proportion/number of labels each class has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55bd57a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.51      0.51       100\n",
      "           1       0.51      0.52      0.52       100\n",
      "\n",
      "    accuracy                           0.52       200\n",
      "   macro avg       0.52      0.52      0.51       200\n",
      "weighted avg       0.52      0.52      0.51       200\n",
      "\n",
      "--------------------------------------------------\n",
      "Multiclass Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.24      0.26      0.25        46\n",
      "           2       0.23      0.20      0.21        46\n",
      "           3       0.34      0.31      0.33        54\n",
      "           4       0.25      0.28      0.26        54\n",
      "\n",
      "    accuracy                           0.27       200\n",
      "   macro avg       0.26      0.26      0.26       200\n",
      "weighted avg       0.27      0.27      0.26       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# here we can see the precision, recall and f1_score for each indivdual label\n",
    "print(\"Binary Classification Report\")\n",
    "report_binary = classification_report(y_true=random_actual, y_pred=random_predicted)\n",
    "print(report_binary)\n",
    "\n",
    "# Multiclass\n",
    "np.random.seed(42)\n",
    "multi_actual = np.random.randint(1,5,size=200)\n",
    "multi_pred = np.random.randint(1,5,size=200)\n",
    "print(\"-\"*50)\n",
    "print(\"Multiclass Classification Report\")\n",
    "report_multi = classification_report(y_true=multi_actual, y_pred=multi_pred)\n",
    "print(report_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8028ca29",
   "metadata": {},
   "source": [
    "## 3. Evaluation example with csv dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
